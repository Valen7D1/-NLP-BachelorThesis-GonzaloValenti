{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKxHoVCqYVZhO2WwcYBwsu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ie-gziVbUPh"},"outputs":[],"source":["from google.colab import drive\n","\n","# mount your google drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/TFG/Exist_2023/\""]},{"cell_type":"code","source":["TASK = \"b\" # \"a\"\n","\n","model_name = \"bert-base-cased\"\n","#model_name = 'bert-base-multilingual-uncased'\n","#model_name = 'bert-base-multilingual-cased'\n","#model_name = \"bert-base-uncased\"\n","#model_name = \"roberta-base\"\n","#model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n","#model_name = \"xlm-roberta-base\"\n","#model_name = \"xlm-roberta-large\"\n","\n","\n","CLASS_FIELD = None\n","\n","# sexism clasification\n","if TASK == \"a\":\n","    NUM_CLASSES = 2\n","    CLASS_FIELD = \"task1\"\n","    remove_columns=['id_tweet','lang','task2', 'size',  'IDEOLOGICAL_INEQUALITY', 'STEREOTYPING_DOMINANCE', 'OBJECTIFICATION', 'MISOGYNY_NON_SEXUALVIOLENCE', 'UNKNOWN', 'NONE', 'SEXUAL_VIOLENCE']\n","\n","\n","# intention clasification\n","elif TASK == \"b\":\n","    NUM_CLASSES = 3\n","    CLASS_FIELD = \"task2\"\n","    remove_columns=['id_tweet','lang','task1', 'size', 'IDEOLOGICAL_INEQUALITY', 'STEREOTYPING_DOMINANCE', 'OBJECTIFICATION', 'MISOGYNY_NON_SEXUALVIOLENCE', 'UNKNOWN', 'NONE', 'SEXUAL_VIOLENCE']\n","\n","\n","\n","print(\"Classification: \", CLASS_FIELD, \", number of classes: \", NUM_CLASSES)"],"metadata":{"id":"jMOipBAbbs_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"BcEd2cO8cF_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install datasets transformers"],"metadata":{"id":"MRBL30qBcQVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --upgrade accelerate"],"metadata":{"id":"bzkltjeJqn8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"csv\", data_files=path+\"train.csv\")\n","val_set = load_dataset(\"csv\", data_files=path+\"validation.csv\")\n","test_set = load_dataset(\"csv\", data_files=path+\"test.csv\")\n","\n","\n","\n","dataset['validation'] = val_set.pop(\"train\")\n","dataset['test'] = test_set.pop(\"train\")\n","dataset"],"metadata":{"id":"5Usd4MnY4CJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset=dataset.remove_columns(remove_columns)\n","dataset"],"metadata":{"id":"RdfkJdg25nGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from every split (test, train, val), if value in task2 column is not \"-\" then include, else exclude\n","if TASK!='a':\n","    # remove rows\n","    for split in dataset.keys():\n","        dataset[split] = dataset[split].select(\n","            (\n","                i for i in range(len(dataset[split]))\n","                    if dataset[split][i][CLASS_FIELD] != '-'\n","            )\n","        )\n","\n","dataset"],"metadata":{"id":"hTHqbK5H5rQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LABELS = sorted(set(dataset[\"train\"][CLASS_FIELD]))\n","print(LABELS)"],"metadata":{"id":"z4ldbFxG5vps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","print('CLASS_FIELD:', CLASS_FIELD)\n","le = LabelEncoder()\n","\n","Y_train=dataset[\"train\"][CLASS_FIELD]\n","\n","y_train = le.fit_transform(Y_train)\n","\n","\n","Y_val=dataset[\"validation\"][CLASS_FIELD]\n","y_val = le.transform(Y_val)\n","\n","Y_test=dataset[\"test\"][CLASS_FIELD]\n","y_test = le.transform(Y_test)\n","\n","\n","\n","try:\n","    dataset['train'] = dataset['train'].add_column(\"label\", y_train)\n","    dataset['validation'] = dataset['validation'].add_column(\"label\", y_val)\n","    dataset['test'] = dataset['test'].add_column(\"label\", y_test)\n","except:\n","    pass\n","\n","dataset"],"metadata":{"id":"kPyV2sbt5x7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset=dataset.remove_columns(CLASS_FIELD)\n","dataset"],"metadata":{"id":"xghi1Cjd51Ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import set_seed, BertTokenizerFast, AutoTokenizer, XLMRobertaXLModel\n","\n","\n","set_seed(42)\n","\n","# load the tokenizer\n","if 'uncased' in model_name:\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, use_fast=True, normalization=True)\n","    # tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, normalization=True)\n","    # tokenizer = BertTokenizerFast.from_pretrained(model_name)"],"metadata":{"id":"s5-VYoX_6Dkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max sequence length for each document/sentence sample\n","MAX_LENGTH = max([len(tokenizer(text).tokens())  for text in dataset['train']['tweet']])\n","# print(MAX_LENGTH)\n","\n","MAX_LENGTH = min(MAX_LENGTH, 512)\n","print('MAX_LENGTH:', MAX_LENGTH)\n"],"metadata":{"id":"q6EtL1DL6Kv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_func(example):\n","    return tokenizer(example[\"tweet\"], truncation=True, padding='max_length',max_length=MAX_LENGTH)\n","\n","encoded_dataset = dataset.map(tokenize_func, batched=True)\n","encoded_dataset"],"metadata":{"id":"z3TSHafq6rvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, TrainingArguments, BertForMultipleChoice, XLMRobertaForSequenceClassification, RobertaForSequenceClassification, XLMRobertaXLModel\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import tensorflow as tf\n","\n","# defines the model and pass to CUDA\n","\n","# model for roberta based models (roberta base, twitter roberta, xlm roberta)\n","# model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True).to(\"cuda\")\n","\n","# model for berta based models (berta base cased/uncased, multilingual)\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).to(\"cuda\")\n","\n","# define arguments\n","args = TrainingArguments(\n","    output_dir='./outputs/',\n","    logging_dir='./logs',            # directory for storing logs\n","\n","    num_train_epochs=1, # 3, we changed to 1 for a faster training. You should increase its value to 3 or 5\n","    evaluation_strategy = \"epoch\",  # \"steps\",   evaluate each `logging_steps`, logging_steps=400,               # log & save weights each logging_steps     save_steps=400,\n","                                    # save_steps=400,\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.05,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"loss\",\n",")\n"],"metadata":{"id":"7WeDmAKn6XdA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define metrics\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n","    acc = accuracy_score(labels, predictions)\n","    return {\n","        'accuracy': acc,    #we could return just the accuracy\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"metadata":{"id":"ta-wcWMx6alj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import concatenate_datasets\n","encoded_train=encoded_dataset[\"train\"]\n","encoded_val=encoded_dataset[\"validation\"]\n","encoded_test=encoded_dataset[\"test\"]\n","\n","encoded_train"],"metadata":{"id":"KjFqGusn6eaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model,  # the model\n","    args,   # the arguments of the model\n","    train_dataset=encoded_train, # the training dataset\n","    eval_dataset=encoded_val, #the validation dataset\n","    tokenizer=tokenizer,    # the tokenizer\n","    compute_metrics=compute_metrics, # the metrics for obtain the metrics on the evaluation\n","    #compute_metrics=evaluate.load(\"accuracy\"),    # metrics are calculated for each epoch\n",")\n","\n","# type(trainer.data_collator)   #transformers.data.data_collator.DataCollatorWithPadding\n","\n","# training\n","trainer.train()"],"metadata":{"id":"91g2iOI_65LV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","SAVE_MODEL = True\n","if SAVE_MODEL:\n","    models_dir = path+'/results'\n","    if not os.path.exists(models_dir): ### If the file directory doesn't already exists,\n","        os.makedirs(models_dir) ### Make it please\n","\n","    model_path = models_dir+model_name+\"_{}\".format(TASK)\n","    model.save_pretrained(model_path)\n","    tokenizer.save_pretrained(model_path)"],"metadata":{"id":"yUPQQokMmzTl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate the current model after training\n","trainer.evaluate()"],"metadata":{"id":"XX9n4Ph6m5Wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_prediction(text):\n","    # prepare our text into tokenized sequence\n","    inputs = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\").to(\"cuda\")\n","    # perform inference to our model\n","    outputs = model(**inputs)\n","    # get output probabilities by doing softmax\n","    probs = outputs[0].softmax(1)\n","    # executing argmax function to get the candidate label\n","    # return probs.argmax() is a tensor. We have to return its item\n","    return probs.argmax().item()"],"metadata":{"id":"Kn9nUwG7m5-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred=[get_prediction(text) for text in dataset['test']['tweet']]"],"metadata":{"id":"i-O0FhlrnAFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, confusion_matrix\n","print(classification_report(y_true=dataset['test']['label'], y_pred=y_pred, target_names=LABELS))\n","cm = confusion_matrix(dataset['test']['label'], y_pred)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n","disp.plot()"],"metadata":{"id":"3X-UT65EnFpu"},"execution_count":null,"outputs":[]}]}