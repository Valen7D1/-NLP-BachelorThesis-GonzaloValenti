{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLwODZqjMUVlCAqMa43Rxk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RHm7KVGdigfI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pip install emoji --upgrade"],"metadata":{"id":"uaqzPuUPijqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","from sklearn import preprocessing\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.model_selection import train_test_split\n","\n","path = \"/content/drive/My Drive/TFG/Exist_2023/\"\n","# we load the training dataset. There is one only file for the three tasks\n","df = pd.read_csv(path+\"all_data.csv\", header=0)\n","\n","print('dataset was loaded')\n","df.head(12)"],"metadata":{"id":"hP3SJscIj13n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import emoji\n","import nltk\n","#import stopwords corpus from nltk\n","from nltk.corpus import stopwords\n","import string #load punctuation charachers\n","\n","# download the packets\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# create the objects\n","stopwrds_eng = set(stopwords.words('english'))\n","stopwrds_esp = set(stopwords.words('spanish'))\n","\n","TAG_RE = re.compile(r'<[^>]+>')\n","\n","def remove_tags(text):\n","    return TAG_RE.sub('', text)\n","\n","def preprocess_text(sen, lang):\n","    #translate emojis\n","    sentence = emoji.demojize(sen)\n","\n","    # stop words elimnation for both espanish and english\n","    if lang == \"es\":\n","      stopwrds = stopwrds_esp\n","    else:\n","      stopwrds = stopwrds_eng\n","\n","    sentence = re.sub(r\"@\\w*\", \" \", str(sentence).lower()).strip() #removing username\n","    sentence = re.sub(r'https?://[A-Za-z0-9./]+', \" \", str(sentence).lower()).strip() #removing links\n","    sentence = re.sub(r'[^a-zA-Z]', \" \", str(sentence).lower()).strip() #removing sp_char\n","\n","\n","    #remove stop words\n","    sentence = ' '.join([x for x in nltk.word_tokenize(sentence) if x not in stopwrds])\n","\n","    # Removing html tags\n","    sentence = remove_tags(sentence)\n","\n","    # Remove punctuations and numbers\n","    #sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n","    sentence = re.sub(r'['+string.punctuation+']',' ',sentence)\n","\n","    # Single character removal\n","    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n","\n","    # Removing multiple spaces\n","    sentence = re.sub(r'\\s+', ' ', sentence)\n","\n","    return sentence.strip()"],"metadata":{"id":"0FkSvTBGikLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_new = []\n","\n","# apply the preprocess to all the dataset\n","for index, row in df.iterrows():\n","    lang = row[\"lang\"]\n","    tweet = row[\"tweet\"]\n","    texts_new.append(remove_tags(preprocess_text(tweet, lang)))\n","\n","# locate the new column in the dataset with the cleaned tweet\n","df['tweet'] = texts_new"],"metadata":{"id":"YH9Hgr2pi6rq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(path+\"train_data_pre.csv\", index=False)\n","\n","df.head(1000)"],"metadata":{"id":"0LYNsCZ2kJ1l"},"execution_count":null,"outputs":[]}]}